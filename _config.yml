# -----------------------------------------------------------------------------
# Site settings
# -----------------------------------------------------------------------------

title: blank
name: ML Evaluation Standards
first_name: ML
middle_name: 
last_name: Evaluation Standards
email:
description: > # the ">" symbol means to ignore newlines until "footer_text:"
  A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
footer_text: >
  Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
  Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.
  Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.
keywords: jekyll, jekyll-theme, academic-website, portfolio-website  # add your own keywords or leave empty

lang: en # the language of your site (for example: en, fr, cn, ru, etc.)
icon: 🔥 # the emoji used as the favicon
url: # the base hostname & protocol for your site
baseurl: # the subpath of your site, e.g. /blog/
last_updated: false # set to true if you want to display last updated in the footer
impressum_path:  # set to path to include impressum link in the footer, use the same path as permalink in a page, helps to conform with EU GDPR

# -----------------------------------------------------------------------------
# RSS Feed
# -----------------------------------------------------------------------------
# will use title and url fields
# Take a look to https://github.com/jekyll/jekyll-feed for more customization

# -----------------------------------------------------------------------------
# Layout
# -----------------------------------------------------------------------------

navbar_fixed: true
footer_fixed: true

# Dimensions
max_width: 800px

# TODO: add layout settings (single page vs. multi-page)

# -----------------------------------------------------------------------------
# Open Graph & Schema.org
# -----------------------------------------------------------------------------
# Display links to the page with a preview object on social media.
serve_og_meta: false # Include Open Graph meta tags in the HTML head
serve_schema_org: false # Include Schema.org in the HTML head
og_image: # The site-wide (default for all links) Open Graph preview image

# -----------------------------------------------------------------------------
# Social integration
# -----------------------------------------------------------------------------

github_username: ml-eval-standards # your GitHub user name
gitlab_username: # your GitLab user name
twitter_username: # your Twitter handle
linkedin_username: # your LinkedIn user name
scholar_userid: # your Google Scholar ID
orcid_id: # your ORCID ID
medium_username: # your Medium username
quora_username: # your Quora username
publons_id: # your ID on Publons
research_gate_profile: # your profile on ResearchGate
blogger_url: # your blogger URL
work_url: # work page URL
keybase_username: # your keybase user name
wikidata_id: # your wikidata id
dblp_url: # your DBLP profile url
stackoverflow_id: #your stackoverflow id

rss_icon: true

contact_note: >
  You can even add a little note about which of these is the best way to reach you.

google_analytics:  # your google-analytics ID (format: UA-XXXXXXXXX)
google_site_verification:  # your google-site-verification ID (Google Search Console)
bing_site_verification:  # out your bing-site-verification ID (Bing Webmaster)
panelbear_analytics:  # panelbear analytics site ID (format: XXXXXXXXX)

# -----------------------------------------------------------------------------
# Blog
# -----------------------------------------------------------------------------

blog_name: # your blog must have a name for it to show up in the nav bar
blog_description: a simple whitespace theme for academics
permalink: /blog/:year/:title/

# Pagination
pagination:
  enabled: true

# Comments
disqus_shortname: al-folio # put your disqus shortname
# https://help.disqus.com/en/articles/1717111-what-s-a-shortname

# -----------------------------------------------------------------------------
# Collections
# -----------------------------------------------------------------------------

collections:
  news:
    defaults:
      layout: post
    output: true
    permalink: /news/:path/
  projects:
    output: true
    permalink: /projects/:path/

news_limit: 5

# -----------------------------------------------------------------------------
# Jekyll settings
# -----------------------------------------------------------------------------

# Markdown and syntax highlight
markdown: kramdown
highlighter: rouge
highlight_theme: github  # https://github.com/jwarby/jekyll-pygments-themes
kramdown:
  input: GFM
  syntax_highlighter_opts:
    css_class: 'highlight'
    span:
      line_numbers: false
    block:
      line_numbers: false
      start_line: 1

# Includes & excludes
include: ['_pages']
exclude:
  - bin
  - Gemfile
  - Gemfile.lock
  - vendor
keep_files:
  - CNAME
  - .nojekyll
  - .git

# Plug-ins
plugins:
  - jekyll-archives
  - jekyll-diagrams
  - jekyll-email-protect
  - jekyll-feed
  - jekyll-github-metadata
  - jekyll-imagemagick
  - jekyll-paginate-v2
  - jekyll/scholar
  - jekyll-sitemap
  - jekyll-target-blank
  - jekyll-twitter-plugin
  - jemoji

# Sitemap settings
defaults:
  - scope:
      path:            "assets/**/*.*"
    values:
      sitemap:         false
# Extras
github: [metadata]

# -----------------------------------------------------------------------------
# Jekyll optimization
# -----------------------------------------------------------------------------

# HTML remove comments (<!-- .... -->)
remove_HTML_comments: false

# HTML beautifier (_plugins/beautify.rb) / https://github.com/threedaymonk/htmlbeautifier
beautify: false # This function has conflict with the code snippets, they can be displayed incorrectly

# HTML minify (_plugins/minify.rb) Thanks to: https://www.ffbit.com/blog/2021/03/17/html-minification-in-jekyll.html
minify: false

# CSS/SASS minify
sass:
  style: compressed

# -----------------------------------------------------------------------------
# Jekyll Archives
# -----------------------------------------------------------------------------

jekyll-archives:
  enabled: [year, tags, categories] # enables year, tag and category archives (remove if you need to disable one of them). 
  layouts:
    year: archive-year
    tag: archive-tag
    category: archive-category
  permalinks:
    year: '/blog/:year/'
    tag: '/blog/tag/:name/'
    category: '/blog/category/:name/'

# -----------------------------------------------------------------------------
# Jekyll Scholar
# -----------------------------------------------------------------------------

scholar:

  last_name: Einstein
  first_name: [Albert, A.]

  style: apa
  locale: en

  source: /_bibliography/
  bibliography: papers.bib
  bibliography_template: bib
  # Note: if you have latex math in your bibtex, the latex filter
  # preprocessing may conflict with MathJAX if the latter is enabled.
  # See https://github.com/alshedivat/al-folio/issues/357.
  bibtex_filters: [latex, smallcaps, superscript]

  replace_strings: true
  join_strings: true

  details_dir: bibliography
  details_layout: bibtex.html
  details_link: Details

  query: "@*"


# -----------------------------------------------------------------------------
# Responsive WebP Images
# -----------------------------------------------------------------------------

imagemagick:
  enabled: false
  widths:
    - 480
    - 800
    - 1400
  input_directories:
    - assets/img
  input_formats:
    - ".jpg"
    - ".jpeg"
    - ".png"
    - ".tiff"
  output_formats:
    webp: "-quality 75%"

# -----------------------------------------------------------------------------
# Jekyll Diagrams
# -----------------------------------------------------------------------------

jekyll-diagrams:
    # configuration, see https://github.com/zhustec/jekyll-diagrams.
    # feel free to comment out this section if not using jekyll diagrams.


# -----------------------------------------------------------------------------
# Optional Features
# -----------------------------------------------------------------------------

enable_google_analytics:    false  # enables google analytics
enable_panelbear_analytics: false  # enables panelbear analytics
enable_google_verification: false  # enables google site verification
enable_bing_verification:   false  # enables bing site verification
enable_mansory:             true   # enables automatic project cards arangement
enable_math:                true   # enables math typesetting (uses MathJax)
enable_tooltips:            false  # enables automatic tooltip links generated
                                   # for each section titles on pages and posts
enable_darkmode:            true   # enables switching between light/dark modes
enable_navbar_social:       false  # enables displaying social links in the
                                   # navbar on the about page
enable_project_categories:  true   # enables categorization of projects into
                                   # multiple categories
enable_medium_zoom:         true   # enables image zoom feature (as on medium.com)


# -----------------------------------------------------------------------------
# Library versions
# -----------------------------------------------------------------------------

academicons:
  version: "1.9.0"
  integrity: "sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg=="
bootstrap:
  version: "4.5.2"
  integrity:
    css: "sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg=="
    js: "sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ=="
fontawesome:
  version: "5.14.0"
  integrity: "sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog=="
jquery:
  version: "3.5.1"
  integrity: "sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg=="
mathjax:
  version: "3.2.0"
mansory:
  version: "4.2.2"
  integrity: "sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI="
mdb:
  version: "4.19.1"
  integrity:
    css: "sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q=="
    js: "sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw=="
popper:
  version: "2.4.4"
  integrity: "sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A=="
medium_zoom:
  version: "1.0.6"
  integrity: "sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM="


# -----------------
# Organizers
# -----------------
organizers: [
    {'name': 'Stephanie Chan',
    'website': 'https://scholar.google.com/citations?hl=en&user=bXOt49QAAAAJ',
    'affiliations': 'DeepMind',
    'img_path': "https://thecatapi.com/api/images/get?format=src&type=gif&timestamp=1"},
    {'name': 'Rishabh Agarwal',
    'website': 'https://agarwl.github.io',
    'affiliations': 'Google Brain',
    'img_path': "https://thecatapi.com/api/images/get?format=src&type=gif&timestamp=2"},
    {'name': 'Xavier Bouthillier',
    'website': 'https://bouthilx.github.io',
    'affiliations': 'Mila, Université de Montréal',
    'img_path': "https://thecatapi.com/api/images/get?format=src&type=gif&timestamp=3"},
    {'name': 'Caglar Gulcehre',
    'website': 'https://caglarg.com',
    'affiliations': 'DeepMind',
    'img_path': "https://thecatapi.com/api/images/get?format=src&type=gif&timestamp=4"},
    {'name': 'Jesse Dodge',
    'website': 'http://www.cs.cmu.edu/~jessed/',
    'affiliations': 'Allen Institute for AI',
    'img_path': "https://thecatapi.com/api/images/get?format=src&type=gif&timestamp=5"},
]

speakers: [
    {'name': 'Thomas Wolf',
    'anchor': 'thomas_wolf',
    'website': 'https://thomwolf.io',
    'affiliations': 'Hugginface Inc.',
    'img_path':"assets/img/thomas_wolf.jpg",
    'time': '12h15',
    'title': 'The challenges of open evaluation at Hugging Face: from open-source evaluation libraries to evaluating +30k hosted models and models of 176B parameters',
    'abstract': "In this talk I'll cover some recent work on projects related to evaluation at Hugging Face where the specificity is to develop open-source and community base solution for solving some of today's challenges in evaluation and reproducibility. From new libraries around metrics to the evaluation of 30k models on the hub up to the evaluation of the 176B parameters model of BigScience.",
    'bio':'<p>Thomas Wolf is co-founder and Chief Science Officer of HuggingFace. The tools created by Thomas Wolf and the HuggingFace team are used across more than 5000 research organisations including Facebook Artificial Intelligence Research, Google Research, DeepMind, Amazon Research, Apple, the Allen Institute for Artificial Intelligence as well as most university departments.</p><p>Thomas Wolf is the initiator and senior chair of the largest research collaboration that has ever existed in Artificial Intelligence: <a href="https://bigscience.huggingface.co">“BigScience”</a> as well as a set of widely used libraries and tools (<a href="https://github.com/huggingface/">huggingface</a>).</p><p>Thomas Wolf is also a prolific educator and a thought leader in the field of Artificial Intelligence and Natural Language Processing, a regular invited speaker to conferences all around the world.</p>'},
    &frank_schneider {'name': 'Frank Schneider',
    'anchor': 'frank_schneider',
    'website': 'http://fsschneider.github.io',
    'affiliations': 'University of Tübingen',
    'title': 'Improving Optimizer Evaluation in Deep Learning',
    'time': '13h00',
    'abstract': "<p>Although hundreds of optimization algorithms have been proposed for deep learning, there is no widely agreed-upon protocol for evaluating their efficiency, performance, and usability. Instead, the crucial choice of the optimizer is too often done based on personal anecdotes instead of grounded empirical evidence.</p><p>In this talk, we present strategies for comparing deep learning optimizers which consider the unique challenges of deep learning such as the inherent stochasticity or the crucial distinction between learning and pure optimization. These strategies are formalized and automatized in the Python package DeepOBS, which allows fairer, faster, and more convincing empirical comparisons of deep learning optimizers.</p><p>Following this protocol, we report insights from our independent, third-party evaluation of the field's current state. A thorough comparison of fifteen popular deep learning optimizers, using roughly 50,000 individual runs, reveals that the comparably traditional Adam optimizer remains a strong but not dominating contender and that newer methods fail to consistently outperform it.</p><p>As an adjacent research direction to benchmarks, new debugging tools, such as Cockpit, allow for a more detailed evaluation of the training process of neural networks beyond just the loss or the model's performance. These tools could disentangle the many factors contributing to (un)successful neural network training, helping us understand whether training improvements are the result of better models, better algorithms, or better hyperparameters.</p>",
    'img_path':"assets/img/frank_schneider.jpg",
    'bio': "Frank Schneider is a Ph.D. student in the Methods of Machine Learning group supervised by Prof. Dr. Philipp Hennig at the University of Tübingen in Germany. His research focuses on making deep learning more user-friendly. He has previously published work on new debugging tools for neural network training and on improving the evaluation process of optimization algorithms for deep learning. He is currently a co-chair of the MLCommons™ Algorithms Working Group. He holds a Bachelor's and Master's degree in Simulation Technology from the University of Stuttgart as well as a Master's degree in Industrial and Applied Mathematics from the Eindhoven University of Technology."},
    # {'name': 'Phillipp Hennig',
    # 'anchor': 'phillipp_hennig',
    # 'website': '',
    # 'affiliations': 'University of Tübingen',
    # 'time': '13h00',
    # 'title': 'TDB',
    # 'abstract': 'TBD',
    # 'img_path':"assets/img/philipp_hennig.jpeg",
    # 'bio': ''},
    &rotem_dror {'name': 'Rotem Dror',
    'anchor': 'rotem_dror',
    'website': 'https://rtmdrr.github.io/',
    'affiliations': 'University of Pennsylvania',
    'img_path':"assets/img/rotem_dror.jpeg",
    'time': '13h45',
    'title': 'A Statistical Analysis of Automatic Evaluation Metrics for Summarization',
    'abstract': '<p>The quality of a summarization evaluation metric is quantified by calculating the correlation between its scores and human annotations across a large number of summaries. Currently, it is not clear how precise these correlation estimates are, nor whether differences between two metrics’ correlations reflect a true difference or if it is due to random chance. In this talk, I will address these two problems by proposing methods for calculating confidence intervals and running hypothesis tests for correlations. After evaluating which of the proposed methods is most appropriate for summarization through two simulation experiments, I will analyze the results of applying these methods to several different automatic evaluation metrics across three sets of human annotations. In this research, we find that the confidence intervals are rather wide, demonstrating high uncertainty in how reliable automatic metrics truly are. Further, although many metrics fail to show statistical improvements over ROUGE, two recent works, QAEval and BERTScore, do in some evaluation settings. This work is published at <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00417/107833/A-Statistical-Analysis-of-Summarization-Evaluation">TACL 2021</a>.</p><p>In the second part of this talk, I will present an ongoing study that identifies two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate summarization systems in practice and propose changes to rectify this disconnect. The results from these analyses point to the need for future research to focus on developing more consistent and reliable human evaluations of summaries.</p><p>This research was done in collaboration with Daniel Deutsch, a Ph.D. student from the Cognitive Computation Group at the Department of Computer and Information Science, University of Pennsylvania.</p>',
    'bio': 'Rotem Dror is a Postdoctoral Researcher at the Cognitive Computation Group at the Department of Computer and Information Science, University of Pennsylvania. She is working with Prof. Dan Roth. She has completed her Ph.D. in the Natural Language Processing Group, supervised by Prof. Roi Reichart, at the Faculty of Industrial Engineering and Management at the Technion - Israel Institute of Technology. In her Ph.D. thesis, she discussed Algorithms and Statistically Sound Evaluation of Structured Solutions in Natural Language Processing. For more information: <a href="https://rtmdrr.github.io/">rtmdrr.github.io</a>.'},
    &james_evans {'name': 'James Evans',
    'anchor': 'james_evans',
    'website': 'https://sociology.uchicago.edu/directory/james-evans',
    'affiliations': 'University of Chicago',
    'time': '16h35',
    'title': 'AI Advance and the Paradox of Sustained Innovation',
    'abstract': 'I briefly survey advances in AI and Deep Learning, considering them in the light of the paradox of sustainable innovation. Growth-oriented economists from Marx to Schumpeter highlighted the process of creative destruction whereby technical successes (like deep learning) displace and render competitors obsolete, but the multi-disciplinary study of innovation reveals a companion process of destructive creation whereby innovations emerge from contexts of discord and disorder, breaches in the structure of prior success. Surveying thousands of fields, millions of teams, and hundreds of millions of scientists, I demonstrate that at all levels, early success comes to be associated with rigidity in education and research, a shift toward the exploitation of prior knowledge, and a demanded performance of continued, incremental success. This represents a shift away from costly exploration, failures and abductive surprise that anticipate punctuated advance at field, team and individual-levels. I simulate processes of disruptive search and show that systematically violating established field boundaries is associated with sizeable successes of future scientific predictions, suggesting diminishing marginal returns to disciplines. I use this as background to consider movements toward open data, the common task framework and evaluation standards to insure ML reproducibility, showing how they broaden the field, but encourage it to collectively learn more about less. I conclude with a discussion of prospects for sustained innovation in ML, which involve rethinking the institution of education as a sustained experiment, the cultivation of diversity, speciation of research tasks and approaches, and the pursuit of productive failure, surfing the interface between order and chaos.',
    'img_path':"assets/img/james_evans.png",
    'bio': "James Evans is the Max Palevsky Professor of History and Civilization in Sociology, Director of Knowledge Lab, and Founding Faculty Director of Computational Social Science at the University of Chicago and the Santa Fe Institute. Evans' research uses large-scale data, machine learning and generative models to understand how collectives think and what they know. This involves inquiry into the emergence of ideas, shared patterns of reasoning, and processes of attention, communication, agreement, and certainty. Thinking and knowing collectives like science, Wikipedia or the Web involve complex networks of diverse human and machine intelligences, collaborating and competing to achieve overlapping aims. Evans' work connects the interaction of these agents with the knowledge they produce and its value for themselves and the system. Evans designs observatories for understanding that fuse data from text, images and other sensors with results from interactive crowd sourcing and online experiments. Much of Evans' work has investigated modern science and technology to identify collective biases, generate new leads taking these into account, and imagine alternative discovery regimes. He has identified R&D institutions that generate more and less novelty, precision, density and robustness. Evans also explores thinking and knowing in other domains ranging from political ideology to popular culture. His work has been published in Nature, Science, PNAS, American Sociological Review, American Journal of Sociology and many other outlets."},
   {'name': 'Melanie Mitchell',
    'anchor': 'melanie_mitchell',
    'website': 'https://melaniemitchell.me',
    'affiliations': 'Sante Fe Institute',
    'time': '18h50',
    'title': 'Beyond Accuracy:  How to Evaluate Understanding on Conceptual Abstraction Benchmarks',
    'abstract': 'The abilities to recognize abstract concepts (e.g. “same” vs. “different”) and make analogies is central to human intelligence, and has received increasing attention in the AI/ML community, with challenge domains such as Raven’s Progressive Matrices, Bongard Problems, and the Abstraction and Reasoning Corpus (ARC).  However, the methods typically used to evaluate ML systems on these domains have failed to assess true abstraction and generalization abilities, and have allowed for “shortcut learning” that succeeds on a particular benchmark, but for the wrong reasons.   In this talk I will propose a different approach to evalution, one that attempts to test for degrees of “understanding” of abstract concepts, beyond simple accuracy measures.',
    'img_path':"assets/img/melanie_mitchell.png",
    'bio': 'Melanie Mitchell is the Davis Professor of Complexity at the Santa Fe Institute. Her current research focuses on conceptual abstraction, analogy-making, and visual recognition in artificial intelligence systems.   Melanie is the author or editor of six books and numerous scholarly papers in the fields of artificial intelligence, cognitive science, and complex systems. Her book Complexity: A Guided Tour (Oxford University Press) won the 2010 Phi Beta Kappa Science Book Award and was named by Amazon.com as one of the ten best science books of 2009. Her latest book is Artificial Intelligence: A Guide for Thinking Humans (Farrar, Straus, and Giroux).'},
   {'name': 'Katherine Heller',
    'anchor': 'katherine_heller',
    'website': '',
    'affiliations': 'Google Brain',
    'time': '19h35',
    'title': 'A framework for improved ML evaluations',
    'abstract': 'I start by discussing ML evaluation goals versus current standards, followed by the presentation of a framework for better addressing these evaluation goals. This framework is comprised of three parts: 1) Qualitiative Evaluation, 2) Demographic Slicing, and 3) Distribution Shift and Causal Evaluations. I present "Healthsheets", a transparency artefact for health datasets, in the spirit of "Datasheets", as work towards improve qualitative evaluation. I will also discuss work on identifying underspecified models, where our model may not encode the causal structure we believe it does, in situations where distribution shift is a factor. We also look at fairness properties in distribution shift situations, and discuss transfer implications of models, when broken down by demographic group. Lastly, I will make a case for the importance of uncertainty and its implications for demographic fairness, discussing work on Electronic Health Record data, Bayesian neural networks, and differences in distributions of mortality predictions for various demographic groups.',
    'img_path':"assets/img/katherine_heller.jpeg",
    'bio': 'Katherine Heller is a Research Scientist in the Responsible AI organization in Google Research. She leads the Context in AI Research (CAIR) group, which focuses on understanding the *context* in which are AI systems are being developed and deployed. Prior to Google, she was faculty in the Statistical Science department at Duke university, where she collaborated across many disciplines to release a sepsis detection system (SepsisWatch), to the Emergency Departments of Duke University hospitals. SepsisWatch has now been run on over 200k patients. She also engaged in many other projects there, through an NSF CAREER, and other awards. Katherine received her PhD from the Gatsby Computational Neuroscience Unit at UCL, and was a postdoctoral fellow at the University of Cambridge and MIT.'},
    &corinna_cortes {'name': 'Corinna Cortes',
    'anchor': 'corinna_cortes',
    'website': 'https://research.google/people/author121/',
    'affiliations': 'Google Research NYC',
    'time': '20h20',
    'title': 'Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment',
    'abstract': '<p>In this talk we revisit the 2014 NeurIPS experiment that examined inconsistency in conference peer review. We determine that 50% of the variation in reviewer quality scores was subjective in origin. Further, with 7+ years passing since the experiment we find that for accepted papers, there is no correlation between quality scores and impact of the paper as measured as a function of citation count. We trace the fate of rejected papers, recovering where these papers were eventually published. For these papers we find a correlation between quality scores and impact. We conclude that the reviewing process for the 2014 conference was good for identifying poor papers, but poor for identifying good papers.</p><p>This 2014 NeurIPS experiment was repeated by the 2021 NeurIPS Program Chairs and we compare the findings. We also discuss other experiments carried out by ICML Program Chairs.</p><p>We hope the findings and some of the ideas from the experiments will help to design a better peer-review pipeline in future conferences.</p>',
    'img_path':"assets/img/corinna_cortes.png",
    'bio':"Corinna Cortes is a VP of Google Research, NY, where she is working on a broad range of theoretical and applied large-scale machine learning problems. Prior to Google, Corinna spent more than ten years at AT&T Labs - Research, formerly AT&T Bell Labs, where she held a distinguished research position. Corinna's research work is well-known in particular for her contributions to the theoretical foundations of support vector machines (SVMs), for which she jointly with Vladimir Vapnik received the 2008 Paris Kanellakis Theory and Practice Award, and her work on data-mining in very large data sets for which she was awarded the AT&T Science and Technology Medal in the year 2000. Corinna received her MS degree in Physics from University of Copenhagen and joined AT&T Bell Labs as a researcher in 1989. She received her Ph.D. in computer science from the University of Rochester in 1993. Corinna is also a competitive runner, and a mother of two."}
]

panels: [
    {'name': 'Reproducibility and Rigor in ML',
    'anchor': 'reproducibility_and_rigor_in_ml',
    'moderator': 'Rishabh Agarwal',
    'description': "Example of questions we would like to cover are: What is reproducibility? What is it useful for and is it necessary for machine learning research? Are we in a different position with respect to other scientific fields because of e.g. the speed of iteration, a culture of open source, and stronger control over our experiments (compared to the natural sciences)? Should we embrace more statistical tools of other scientific domains, or are current  benchmarking methods sufficiently reliable? How much rigor is needed for ML?",
    'panelists': [
        *rotem_dror,
        {'name': 'Sara Hooker',
        'website': 'https://www.sarahooker.me/',
        'affiliations': 'Google Brain',
        'img_path':"assets/img/sara_hooker.png",
        'bio': 'Sara Hooker is a research scholar at Google Brain. Her research interests include interpretability, model compression and security in deep neural networks. In 2014, Sara founded Delta Analytics, a non-profit dedicated to building technical capacity to help communities across the world use machine learning. In 2014, she founded Delta Analytics, a non-profit dedicated to bringing technical capacity to help non-profits across the world use machine learning for good. She grew up in Mozambique, Lesotho, Swaziland, South Africa, and Kenya and currently resides in California.'},
        {'name': 'Koustuv Sinha',
        'website': 'https://www.cs.mcgill.ca/~ksinha4/',
        'affiliations': 'Mila, McGill University',
        'img_path':"assets/img/koustuv_sinha.jpg",
        'bio': 'Koustuv Sinha is a PhD Candidate at McGill University / Mila, supervised by Joelle Pineau. Koustuv’s research focuses on investigating systematicity in natural language understanding (NLU) models, especially the state-of-the-art large language models. His research goal is to develop methods to analyze the failure cases in robustness and systematicity of these NLU models, and develop methods to alleviate them in production. He is the organizer of the annual ML Reproducibility Challenge since 2018, and serves as an associate editor in ReScience journal. He has also served as Reproducibility Chair at NeurIPS in 2019 and 2020'},
        *frank_schneider,
        {'name': 'Gaël Varoquaux',
        'website': 'http://gael-varoquaux.info/',
        'affiliations': 'INRIA',
        'img_path':"assets/img/gael_varoquaux.jpg",
        'bio': 'Gaël Varoquaux is a research director working on data science and health at Inria (French Computer Science National research). His research focuses on statistical-learning tools for data science and scientific inference, with an eye on applications in health and social science. He develops tools to make machine learning easier, with statistical models suited for real-life, uncurated data, and software for data science. For example, since 2008, he has been exploring data-intensive approaches to understand brain function and mental health. He co-funded scikit-learn, one of the reference machine-learning toolboxes, and helped build various central tools for data analysis in Python. Varoquaux has a PhD in quantum physics and is a graduate from Ecole Normale Superieure, Paris.'}
        ]
    },
    {'name': 'Slow vs Fast Science',
    'anchor': 'slow_vs_fast_science',
    'moderator': 'Xavier Bouthillier',
    'description': "Examples of questions we would like to cover are: Slow or fast science, is this a false dichotomy? Is the slow-science movement a threat for exploration? Or is the current pace of publication becoming overwhelming, harming the dissemination of knowledge? Should we look for incentives to balance so-called ‘slow’ and ‘fast’ approaches and if so how could we evaluate what would be a proper balance?",
    'panelists': [
        {'name': 'Chelsea Finn',
        'website': 'https://ai.stanford.edu/~cbfinn/',
        'affiliations': 'Stanford University',
        'img_path':"assets/img/chelsea_finn.jpg",
        'bio': "Chelsea Finn is an Assistant Professor in Computer Science and Electrical Engineering at Stanford University, and the William George and Ida Mary Hoover Faculty Fellow. Finn's research interests lie in the capability of robots and other agents to develop broadly intelligent behavior through learning and interaction. To this end, her work has included deep learning algorithms for concurrently learning visual perception and control in robotic manipulation skills, inverse reinforcement methods for learning reward functions underlying behavior, and meta-learning algorithms that can enable fast, few-shot adaptation in both visual perception and deep reinforcement learning. Finn received her Bachelor's degree in Electrical Engineering and Computer Science at MIT and her PhD in Computer Science at UC Berkeley. Her research has been recognized through the Microsoft Research Faculty Fellowship, the IEEE RAS Early Academic Career Award, the ONR Young Investigator Award, the ACM doctoral dissertation award, and the MIT Technology Review 35 under 35 Award, and her work has been covered by various media outlets, including the New York Times, Wired, and Bloomberg. Throughout her career, she has sought to increase the representation of underrepresented minorities within CS and AI by developing an AI outreach camp at Berkeley for underprivileged high school students, a mentoring program for underrepresented undergraduates across four universities, and leading efforts within the WiML and Berkeley WiCSE communities of women researchers."},
        {'name': 'Michela Paganini',
        'website': 'https://mickypaganini.github.io',
        'affiliations': 'DeepMind',
        'img_path':"assets/img/michela_paganini.jpeg",
        'bio': 'Michela is a Research Scientist at DeepMind. She was previously a Postdoctoral Researcher at Facebook AI Research and an affiliate at Lawrence Berkeley National Lab. She earned her Ph.D. in physics from Yale University, where she worked on the design, development, and deployment of deep learning algorithms for the ATLAS experiment at CERN, with a focus on computer vision and generative modeling. Prior to that, she graduated from the University of California, Berkeley with degrees in physics and astrophysics. Her current research focuses on model understanding and sparsification: her work involves empirically characterizing neural network behavior, with a recent interest in large scale language models, by investigating their inner workings in the over-parametrized and under-parametrized regimes. Michela has a broad interest in the science of deep learning, with a focus on understanding emergent behavior in neural networks from a mechanistic perspective.'},
        *james_evans,
        {'name': 'Russel Poldrack',
        'website': 'https://poldracklab.stanford.edu',
        'affiliations': 'Stanford University',
        'img_path':"assets/img/russel_poldrack.jpg",
        'bio': 'Russell Poldrack is a Professor in the Stanford Department of Psychology, Associate Director of Stanford Data Science, and Director of the Center for Open and Reproducible Science (CORES). His laboratory’s basic research focuses on understanding the brain systems involved in decision making and self control in humans using neuroimaging and behavioral methods. The laboratory has also developed a number of resources for open and reproducible science, including the OpenNeuro data sharing platform and the fMRIPrep preprocessing workflow.'},
        {'name': 'Oriol Vinyals',
        'website': 'https://research.google/people/OriolVinyals/',
        'affiliations': 'DeepMind',
        'img_path':"assets/img/oriol_vinyals.jpg",
        'bio': 'Oriol Vinyals is a Principal Scientist at Google DeepMind, and a team lead of the Deep Learning group. His work focuses on Deep Learning and Artificial Intelligence. Prior to joining DeepMind, Oriol was part of the Google Brain team. He holds a Ph.D. in EECS from the University of California, Berkeley and is a recipient of the 2016 MIT TR35 innovator award. His research has been featured multiple times at the New York Times, Financial Times, WIRED, BBC, etc., and his articles have been cited over 70000 times. His academic involvement includes program chair for the International Conference on Learning Representations (ICLR) of 2017, and 2018. He has also been an area chair for many editions of the NeurIPS and ICML conferences. Some of his contributions such as seq2seq, knowledge distillation, or TensorFlow are used in Google Translate, Text-To-Speech, and Speech recognition, serving billions of queries every day, and he was the lead researcher of the AlphaStar project, creating an agent that defeated a top professional at the game of StarCraft, achieving Grandmaster level, also featured as the cover of Nature. At DeepMind he continues working on his areas of interest, which include artificial intelligence, with particular emphasis on machine learning, deep learning and reinforcement learning.'}
        ]
    },
    {'name': 'Incentives for Better Evaluation',
    'anchor': 'incentives_for_better_eval',
    'moderator': 'Stephanie Chan',
    'description': "Examples of questions we would like to cover are: What are the pain-points for which we would need new/better incentives to improve the situation? What roles are the conferences and journals playing to improve these pain-points? Are we investing enough effort to monitor and identify issues in the review process? Is it acceptable to increase the workload of reviewers and ACs in order to gather data about the review process? Would a tighter loop between research and production lead to greater accountability?",
    'panelists': [
        *corinna_cortes,
        {'name': 'Yoshua Bengio',
        'website': 'https://yoshuabengio.org/',
        'affiliations': 'Mila, Université de Montréal',
        'img_path':"assets/img/yoshua_bengio.jpg",
        'bio': 'Recognized worldwide as one of the leading experts in artificial intelligence, Yoshua Bengio is most known for his pioneering work in deep learning, earning him the 2018 A.M. Turing Award, “the Nobel Prize of Computing,” with Geoffrey Hinton and Yann LeCun. He is a Full Professor at Université de Montréal, and the Founder and Scientific Director of Mila – Quebec AI Institute. He co-directs the CIFAR Learning in Machines & Brains program as Senior Fellow and acts as Scientific Director of IVADO. In 2019, he was awarded the prestigious Killam Prize and in 2021, became the second most cited computer scientist in the world. He is a Fellow of both the Royal Society of London and Canada, Knight of the Legion of Honor of France and Officer of the Order of Canada. Concerned about the social impact of AI and the objective that AI benefits all, he actively contributed to the Montreal Declaration for the Responsible Development of Artificial Intelligence.'},
        {'name': 'John Langford',
        'website': 'https://hunch.net',
        'affiliations': 'Microsoft Research',
        'img_path':"assets/img/john_langford.jpg",
        'bio': "John Langford is a computer scientist working in machine learning and learning theory. He is well known for work on the Isomap embedding algorithm, CAPTCHA challenges, Cover Trees for nearest neighbor search, Contextual Bandits for reinforcement learning applications, and learning reductions. John is the author of the blog hunch.net and the principal developer of Vowpal Wabbit. He works at Microsoft Research New York, of which he was one of the founding members, and was previously affiliated with Yahoo! Research, Toyota Technological Institute at Chicago, and IBM's Watson Research Center. He studied Physics and Computer Science at the California Institute of Technology, earning a double bachelor's degree in 1997, and he received his Ph.D. in Computer Science from Carnegie Mellon University in the year of 2002. John was the program co-chair for the 2012 International Conference on Machine Learning (ICML), general chair for the 2016 ICML, and is the President of ICML from 2019–2021."},
        {'name': 'Kyunghyun Cho',
        'website': 'https://kyunghyuncho.me/',
        'affiliations': 'New York University',
        'img_path':"assets/img/kyunghyun_cho.jpeg",
        'bio': 'Kyunghyun Cho is an associate professor of computer science and data science at New York University and CIFAR Fellow of Learning in Machines & Brains. He is also a senior director of frontier research at the Prescient Design team within Genentech Research & Early Development (gRED). He was a research scientist at Facebook AI Research from June 2017 to May 2020 and a postdoctoral fellow at University of Montreal until Summer 2015 under the supervision of Prof. Yoshua Bengio, after receiving PhD and MSc degrees from Aalto University April 2011 and April 2014, respectively, under the supervision of Prof. Juha Karhunen, Dr. Tapani Raiko and Dr. Alexander Ilin.'}
        ]
    }
]


accepted_orals: [
    {
        'id': 1,
        'authors': 'Edward  Raff',
        'title': 'Does the Market of Citations Reward Reproducible Work?',
        'abstract': 'The field of bibliometrics, studying citations and behavior, is critical to the the discussion of reproducibility as citations are one of the primary incentive and reward systems for academic work. Yet to the best of our knowledge, only one work has attempted to look at this combined space, concluding that non-reproducible work is more highly cited. We show that answering this question is more challenging than first proposed, and subtle issues can inhibit a robust conclusion. To make inferences with more robust behavior, we propose a hierarchical Bayesian model that incorporates the citation rate over time, rather than the total number of citations after a fixed amount of time. In doing so we show that, under current evidence the the answer is more likely that certain fields of study such as Medicine and Machine Learning (ML) do reward reproducible works with more citations, but other fields appear to have no relationship. Further, we find that making code available and thoroughly referencing prior works appear to also positively correlate with increased citations.',
        'pdf': "assets/pdf/Citation_Rate_and_Reproduction_Journal.pdf",
    },
    {
        'id': 2,
        'authors': 'Edward Raff & Andrew Farris',
        'title': 'A Siren Song of Open Source Reproducibility',
        'abstract': 'As reproducibility becomes a greater concern, conferences have largely converged to a strategy of asking reviewers to indicate whether code was attached to a submission. This is part of a larger trend of taking action based on assumed ideals, without studying if those actions will yield the desired outcome. Our argument is that this focus on code for replication is misguided if we want to improve the state of reproducible research. This focus can be harmful --- we should not force code to be submitted. Due to the lack of evidence for actions taken today, we argue that it is clear that conferences must do more to encourage and reward the study of reproducibility itself, so that we can learn what actions should be taken.',
        'pdf': "assets/pdf/The_Risk_of_Open_Source.pdf",
    },
    {
        'id': 3,
        'authors': 'John Kirchenbauer, Jacob Oaks & Eric Heim',
        'title': 'What is Your Metric Telling You? Evaluating Classifier Calibration under Context-Specific Definitions of Reliability',
        'abstract': 'Classifier calibration has received recent attention from the machine learning community due both to its practical utility in facilitating decision making, as well as the observation that modern neural network classifiers are poorly calibrated. Much of this focus has been towards the goal of learning classifiers such that their output with largest magnitude (the “predicted class”) is calibrated. However, this narrow interpretation of classifier outputs does not adequately capture the variety of practical use cases in which classifiers can aid in decision making. In this work, we argue that more expressive metrics must be developed that accurately measure calibration error for the specific context in which a classifier will be deployed. To this end, we derive a number of different metrics using a generalization of Expected Calibration Error (ECE) that measure calibration error under different definitions of reliability. We then provide an extensive empirical evaluation of commonly used neural network architectures and calibration techniques with respect to these metrics. We find that: 1) definitions of ECE that focus solely on the predicted class fail to accurately measure calibration error under a selection of practically useful definitions of reliability and 2) many common calibration techniques fail to improve calibration performance uniformly across ECE metrics derived from these diverse definitions of reliability.',
        'pdf': "assets/pdf/Calibration_Evaluation.pdf",
    },
    {
        'id': 1,
        'authors': '',
        'title': '',
        'abstract': '',
        'pdf': "assets/pdf/",
    }

]

accepted_posters: [
    {
        'authors': 'Edward  Raff',
        'title': 'Does the Market of Citations Reward Reproducible Work?',
        'pdf': "assets/pdf/Citation_Rate_and_Reproduction_Journal.pdf",
    }
]


papers: [
    {"id": 1, 
    "oral": "false",
    "title": "Does the Market of Citations Reward Reproducible Work?", 
    "abstract": "The field of bibliometrics, studying citations and behavior, is critical to the the discussion of reproducibility as citations are one of the primary incentive and reward systems for academic work. Yet to the best of our knowledge, only one work has attempted to look at this combined space, concluding that non-reproducible work is more highly cited. We show that answering this question is more challenging than first proposed, and subtle issues can inhibit a robust conclusion. To make inferences with more robust behavior, we propose a hierarchical Bayesian model that incorporates the citation rate over time, rather than the total number of citations after a fixed amount of time. In doing so we show that, under current evidence the the answer is more likely that certain fields of study such as Medicine and Machine Learning (ML) do reward reproducible works with more citations, but other fields appear to have no relationship. Further, we find that making code available and thoroughly referencing prior works appear to also positively correlate with increased citations. ", 
    "authors": "Edward Raff", 
    "pdf": "Citation_Rate_and_Reproduction_Journal.pdf",
    "video": "https://youtu.be/DRCs9CxyDoo",
    "poster": "https://www.edwardraff.com/publications/iclr_eval_reproducible_citation_poster.pdf"}, 
    {"id": 2,
    "title": "A Siren Song of Open Source Reproducibility", "abstract": "As reproducibility becomes a greater concern, conferences have largely converged to a strategy of asking reviewers to indicate whether code was attached to a submission. This is part of a larger trend of taking action based on assumed ideals, without studying if those actions will yield the desired outcome. Our argument is that this focus on code for replication is misguided if we want to improve the state of reproducible research. This focus can be harmful --- we should not force code to be submitted. Due to the lack of evidence for actions taken today, we argue that it is clear that conferences must do more to encourage and reward the study of reproducibility itself, so that we can learn what actions should be taken. ", 
    "authors": "Edward Raff & Andrew L Farris", 
    "pdf": "The_Risk_of_Open_Source.pdf",
    "video": "https://youtu.be/aA6HGZLcB2Y",
    "oral": "true",
    "poster": "https://www.edwardraff.com/publications/iclr_eval_siren_song_poster.pdf"}, 
    {"id": 3, 
    "oral": "false",
    "title": "What is Your Metric Telling You? Evaluating Classifier Calibration under Context-Specific Definitions of Reliability", "abstract": "Classifier calibration has received recent attention from the machine learning community due both to its practical utility in facilitating decision making, as well as the observation that modern neural network classifiers are poorly calibrated. Much of this focus has been towards the goal of learning classifiers such that their output with largest magnitude (the \u201cpredicted class\u201d) is calibrated. However, this narrow interpretation of classifier outputs does not adequately capture the variety of practical use cases in which classifiers can aid in decision making. In this work, we argue that more expressive metrics must be developed that accurately measure calibration error for the specific context in which a classifier will be deployed. To this end, we derive a number of different metrics using a generalization of Expected Calibration Error (ECE) that measure calibration error under different definitions of reliability. We then provide an extensive empirical evaluation of commonly used neural network architectures and calibration techniques with respect to these metrics. We find that: 1) definitions of ECE that focus solely on the predicted class fail to accurately measure calibration error under a selection of practically useful definitions of reliability and 2) many common calibration techniques fail to improve calibration performance uniformly across ECE metrics derived from these diverse definitions of reliability.", 
    "authors": "John Kirchenbauer, Jacob R Oaks & Eric Heim", 
    "pdf": "Calibration_Evaluation.pdf",
    "video": "https://youtu.be/FPQrN9shsKc",
    "poster": "https://drive.google.com/file/d/1H4sBbYhX5TJEJ7-ymnytlaYPV09F8KAD/view"}, 
    {"id": 4, 
    "oral": "false",
    "title": "System Analysis for Responsible Design of Modern AI/ML Systems", "abstract": "The irresponsible use of ML algorithms in practical settings has received a lot of deserved attention in the recent years. We posit that the traditional system analysis perspective is needed when designing and implementing ML algorithms and systems. Such perspective can provide a formal way for evaluating and enabling responsible ML practices. In this paper, we review components of the System Analysis methodology and highlight how they connect and enable responsible practices of ML design.", 
    "authors": "Virginia Goodwin & Rajmonda S. Caceres", 
    "pdf": "SystemAnalysisAI_Short.pdf",
    "video": "",
    "poster": ""}, 
    {"id": 6, 
    "oral": "false",
    "title": "A Brief Guide to Designing and Evaluating Human-Centered Interactive Machine Learning", "abstract": "Interactive machine learning (IML) is a field of research that explores how to leverage both human and computational abilities in decision making systems. IML represents a collaboration between multiple complimentary human and machine intelligent systems working as a team, each with their own unique abilities and limitations. This teamwork might mean that both systems take actions at the same time, or in sequence. Two major open research questions in the field of IML are: ``How should we design systems that can learn to make better decisions over time with human interaction?'' and ``How should we evaluate the design and deployment of such systems?'' A lack of appropriate consideration for the humans involved can lead to problematic system behaviour, and issues of fairness, accountability, and transparency. Thus, our goal with this work is to present a human-centred guide to designing and evaluating IML systems while mitigating risks. This guide is intended to be used by machine learning practitioners who are responsible for the health, safety, and well-being of interacting humans. An obligation of responsibility for public interaction means acting with integrity, honesty, fairness, and abiding by applicable legal statutes. With these values and principles in mind, we as a machine learning research community can better achieve goals of augmenting human skills and abilities. This practical guide therefore aims to support many of the responsible decisions necessary throughout the iterative design, development, and dissemination of IML systems.", 
    "authors": "Kory  W. Mathewson & Patrick Pilarski", 
    "pdf": "Designing_and_Evaluating_Human_Centered_Interactive_Machine_Learning.pdf",
    "video": "https://www.youtube.com/watch?v=D7wwd5u7Ns4",
    "poster": ""}, 
    {"id": 7, 
    "oral": "false",
    "title": "deep-significance - Easy and Meaningful Statistical Significance Testing in the Age of Neural Networks", "abstract": "A lot of Machine Learning (ML) and Deep Learning (DL) research is of an empirical nature. Nevertheless, statistical significance testing (SST) is still not widely used. This endangers true progress, as seeming improvements over a baseline might be statistical flukes, leading follow-up research astray while wasting human and computational resources. Here, we provide an easy-to-use package containing different significance tests and utility functions specifically tailored towards research needs and usability.", 
    "authors": "Dennis Ulmer, Christian Hardmeier & Jes Frellsen", 
    "pdf": "deep_significance_paper.pdf",
    "video": "https://recorder-v3.slideslive.com/?share=64717&s=9c408722-a4e3-4934-8da8-4083c5a20564",
    "poster": "https://www.dropbox.com/s/nblo9h5ohfmbozq/mles-deep-significance-poster_v2.pdf?dl=0"}, 
    {"id": 8, 
     "oral": "true",
    "title": "Experimental Standards for Deep Learning Research: A Natural Language Processing Perspective", "abstract": "The field of Deep Learning (DL) has undergone explosive growth during the last decade, with a substantial impact on Natural Language Processing (NLP) as well. Yet, as with other fields employing DL techniques, there has been a lack of common experimental standards compared to more established disciplines. Starting from fundamental scientific principles, we distill ongoing discussions on experimental standards in DL into a single, widely-applicable methodology. Following these best practices is crucial to strengthening experimental evidence, improve reproducibility and enable scientific progress. These standards are further collected in a public repository to help them transparently adapt to future needs.", 
    "authors": "Dennis Ulmer, Elisa Bassignana, Max Müller-Eberstein, Daniel Varab, Mike Zhang, Christian Hardmeier & Barbara Plank", 
    "pdf": "Experimental_Standards_Paper.pdf",
    "video": "https://youtu.be/7xL0zrINx64",
    "poster": "https://www.dropbox.com/s/i4zqbbro48cxejr/poster-experime%20ntal-standards.pdf?dl=0"}, 
    {"id": 9, 
    "oral": "false",
    "title": "Reproducible Subjective Evaluation",
    "abstract": "Human perceptual studies are the gold standard for the evaluation of many research tasks in machine learning, linguistics, and psychology. However, these studies require significant time and cost to perform. As a result, many researchers use objective measures that can correlate poorly with human evaluation. When subjective evaluations are performed, they are often not reported with sufficient detail to ensure reproducibility. We propose Reproducible Subjective Evaluation (ReSEval), an open-source framework for quickly deploying crowdsourced subjective evaluations directly from Python. ReSEval lets researchers launch A/B, ABX, Mean Opinion Score (MOS) and MUltiple Stimuli with Hidden Reference and Anchor (MUSHRA-style)  tests on audio, image, text, or video data from a command-line interface or using one line of Python, making it as easy to run as objective evaluation. With ReSEval, researchers can reproduce each other's subjective evaluations by sharing a configuration file and the audio, image, text, or video files.", 
    "authors": "Max Morrison, Brian Tang, Gefei Tan & Bryan Pardo", 
    "pdf": "Reproducible_Subjective_Evaluation.pdf",
    "video": "https://www.youtube.com/watch?v=Usbn4TVtALE",
    "poster": "https://drive.google.com/file/d/1ohlBMgt1MqVvRXjLSt988oxlMSPssLbn/view?usp=sharing"}, 
    {"id": 10, 
    "oral": "false",
    "title": "Increasing Confidence in Adversarial Robustness Evaluations",
    "abstract": "Hundreds of defenses have been proposed in the past years to make deep neural networks robust against minimal (adversarial) input perturbations. However, only a handful of these could hold up their claims because correctly evaluating robustness is extremely challenging: Weak attacks often fail to find adversarial examples even if they unknowingly exist, thereby making a vulnerable network look robust. In this paper, we propose a test to identify weak attacks. Our test introduces a small and simple modification into a neural network that guarantees the existence of an adversarial example for every sample. Consequentially, any correct attack must succeed in attacking this modified network. For eleven out of thirteen previously-published defenses, the original evaluation of the defense fails our test while stronger attacks that break these defenses pass it. We hope that attack unit tests such as ours will be a major component in future robustness evaluations and increase confidence in an empirical field that today is riddled with skepticism and disbelief.", 
    "authors": "Roland S. Zimmermann, Wieland Brendel, Florian Tramer & Nicholas Carlini", 
    "pdf": "Increasing_Confidence_in_Adversarial_Robustness_Evaluations.pdf",
    "video": "",
    "poster": "https://drive.google.com/file/d/1zvLUmWEKCHyRq01tgJpk30ZK_cG90Tpw/view?usp=sharing"}, 
    {"id": 11, 
    "oral": "false",
    "title": "Machine Learning State-of-the-Art with Uncertainties", "abstract": "With the availability of data, hardware, software ecosystem and relevant skill sets, the machine learning community is undergoing a rapid development with new architectures and approaches appearing at high frequency every year. In this article, we conduct an exemplary image classification study in order to demonstrate how confidence intervals around accuracy measurements can greatly enhance the communication of research results as well as impact the reviewing process. In addition, we explore the hallmarks and limitations of this approximation. We discuss the relevance of this approach reflecting on a spotlight publication of ICLR22. A reproducible workflow is made available as open-source adjoint to this publication. Based on our discussion, we make suggestions for improving the authoring and reviewing process of machine learning articles.", 
    "authors": "Peter Steinbach, Felicita Gernhardt, Mahnoor Tanveer, Steve Schmerler & Sebastian Starke", 
    "pdf": "2204.05173.pdf",
    "video": "https://drive.google.com/file/d/1poaeX-l9qeUlos09kkYoxB8uh9bJv1as/view?usp=sharing",
    "poster": "https://doi.org/10.6084/m9.figshare.19597444.v1"}, 
    {"id": 13, 
    "oral": "true",
    "title": "A Case for Better Evaluation Standards in NLG", "abstract": "Evaluating natural language generation (NLG) models has become a popular and active field of study, which has led to the release of novel datasets, automatic metrics, and human evaluation methods. Yet, newly established best practices are often not adopted. Moreover, the research process is often hindered by the scarcity of released resources like model outputs, and a lack of documentation of evaluation parameters often complicates judging new NLG methods. We analyze 66 papers published in 2021 across 29 different dimensions to quantify this effect, and identify promising ways for the research community to improve reporting and reviewing experimental results.", 
    "authors": "Sebastian Gehrmann, Elizabeth Clark & Thibault Sellam", 
    "pdf": "better_eval_in_NLG.pdf",
    "video": "",
    "poster": ""}, 
    {"id": 14, 
    "oral": "false",
    "title": "Towards Clear Expectations for Uncertainty Estimation", "abstract": "If Uncertainty Quantification (UQ) is crucial to achieve trustworthy ML, most UQ\nmethods suffer from disparate and inconsistent evaluation protocols.\nWe claim this inconsistency results from the uncleared requirements the community expects from UQ. This opinion paper offers a new perspective by specifying those requirements through five downstream tasks where we expect uncertainty scores to have substantial predictive power. On an example benchmark of 7 classification datasets, we did not observe statistical superiority of state-of-the-art intrinsic UQ methods against simple baselines. We believe that our findings question the very rationale of why we quantify uncertainty and call for a standardized protocol for UQ evaluation based on metrics proven to be relevant for the ML practitioner.", 
    "authors": "Victor Bouvier, Simona Maggio, Alexandre Abraham & Léo Dreyfus-Schmidt", 
    "pdf": "paper_14_final.pdf",
    "video": "https://drive.google.com/file/d/1p9Yn4Wy6DBx1YgGf5axA6sFo5EjkiUb9/view?usp=sharing",
    "poster": "https://drive.google.com/file/d/1oDW3z7FM8upDwb70rrQ_LR35Oke8hELv/view?usp=sharing"}, 
    {"id": 15, 
    "oral": "false",
    "title": "Rethinking Machine Learning Model Evaluation in Pathology", "abstract": "Machine Learning has been applied to pathology images in research and clinical practice with promising outcomes. However, standard ML models often lack the high quality or rigorous evaluation required for clinical decisions. Most of these models trained on natural images are also ill-equipped to deal with pathology images that are significantly large and noisy, require expensive labeling, are hard to interpret, and are susceptible to spurious correlations. We propose a set of highly relevant and practical guidelines for ML evaluations in pathology that address the above concerns. The paper includes measures for setting up the evaluation framework, efficiently dealing with variability in labels, and a recommended suite of tests to address issues related to domain shift, robustness, and confounding variables. We hope that the proposed framework will bridge the gap between ML researchers and domain experts, leading to wider adoption of ML techniques in pathology and improving patient outcomes.", 
    "authors": "Syed Ashar Javed, Dinkar Juyal, Shreya Chakraborty, Zahil Shanis, Harsha Pokkalla & Aaditya Prakash",
    "pdf": "Evaluation_of_ML_for_Pathology.pdf",
    "video": "https://recorder-v3.slideslive.com/#/share?share=64754&s=ee14c0e6-4625-4868-8d4a-d38aa062dda5",
    "poster": "https://drive.google.com/file/d/1lo6VqcrOHIaKOu1ucicv5Knrh6j45GWn/view?usp=sharing"}, 
    {"id": 16, 
    "oral": "false",
    "title": "Strengthening Subcommunities: Towards Sustainable Growth in AI Research", "abstract": "AI\u2019s rapid growth has been felt acutely by scholarly venues, leading to growing pains within the peer review process. These problems largely center on the inability of specific sub-areas to identify and evaluate work that is appropriate according to criteria relevant to each subcommunity as determined by stakeholders of that sub-area. We set forth a proposal that re-focuses efforts within these subcommunities through a decentralization of the reviewing and publication process. Through this re-centering effort, we hope to encourage each sub-area to confront the issues specific to their process of academic publication and incentivization. This model has been successful for several subcommunities in AI, and we highlight those instances as examples for how the broader field can continue to evolve despite its continually growing size.", 
    "authors": "Andi Peng, Jessica Zosa Forde, Yonadav Shavit & Jonathan Frankle", 
    "pdf": "ML_Evaluation_Submission.pdf",
    "video": "",
    "poster": ""}, 
    {"id": 17, 
    "oral": "false",
    "title": "A meta analysis of data-driven newsvendor approaches", "abstract": "Recently, a number of publications in leading operations management and operations research journals proposed new models that combine machine learning and mathematical optimization techniques to predict inventory decisions directly from historical demand and additional feature information. This paper presents the results of a meta analysis of recent machine learning based approaches for solving the most prominent problem in operations management, the newsvendor problem. We find that the reproducibility of existing studies is typically low, because authors evaluate their new approaches based on small and proprietary datasets, do not share data and code, and use different benchmarks. We develop a reproducible, unified evaluation procedure and apply various models to a large and heterogeneous dataset. Our results do not support the findings and claims of most of the recent papers and, in several cases, we even obtain contradicting results. Overall, the robustness of the newly proposed models appears to be low. To support both researchers and practitioners in the development and evaluation of new models, we provide extensive benchmark data and a python library that contains open source implementations of most existing models.", 
    "authors": "Simone Buttler, Andreas Philippi, Nikolai Stein & Richard Pibernik", 
    "pdf": "ICLR22_Workshop_ML_Eval_DDNV.pdf",
    "video": "https://drive.google.com/file/d/18MrZgXOVyg5nk7HQuE7LBCE0Ec3Bl4hE/view?usp=sharing",
    "poster": "https://drive.google.com/file/d/1qgVSI9Gz9BWzueX__1qAAlbT81eR-k5e/view?usp=sharing"}, 
    {"id": 18, 
    "oral": "false",
    "title": "CheckDST: Measuring Real-World Generalization of Dialogue State Tracking Performance", "abstract": "Neural models that extend the pretrain-then-finetune paradigm continue to achieve new state-of-the-art results in dialogue state tracking (DST) benchmarks on joint goal accuracy (JGA). However, motivated by CheckList (Ribeiro et al. 2020), we argue for a holistic assessment of DST models since JGA is unable to capture robustness to the inevitable test-time distribution shifts. To this end, we build on recent work on robustness testing in task-oriented dialogue and introduce CheckDST, an instantiation of CheckList for DST that quantifies robustness with test set augmentations and new metrics that measure consistency. Using CheckDST, we are able to extensively compare state-of-the-art DST models, finding that, although span-based classification models achieve slightly better JGA on the original test set than generation models, they are significantly less robust to distribution shift. Secondly, we observe that while stopping training early, e.g. at the first epoch, hurts JGA, the resulting models are significantly more robust to distribution shift. Lastly, guided by the weaknesses exposed by CheckDST, we explore training DST models that simultaneously boost JGA and CheckDST metrics and report preliminary success with PrefineDST, a simple generation model pretrained with non-target datasets to internalize reasoning skills relevant to dialogue state tracking.", 
    "authors": "Hyundong Cho, Chinnadhurai Sankar, Christopher Lin, Kaushik Ram Sadagopan, Shahin Shayandeh, Asli Celikyilmaz, Jonathan May & Ahmad Beirami", 
    "pdf": "CheckDST_and_PrefineDST.pdf",
    "video": "https://www.loom.com/share/adcdfb8a39f244d69d8b07ec613c701b",
    "poster": "https://drive.google.com/file/d/1rdQFsEGUJ7NhqPuRMQP3R4e2ANM0X-zt/view?usp=sharing"},
    {"id": 19, 
    "oral": "false",
    "title": "A Survey On Uncertainty Toolkits For Deep Learning",
    "abstract": "The success of deep learning (DL) fostered the creation of unifying frameworks such as tensorflow or pytorch as much as it was driven by their creation in return. Having common building blocks facilitates the exchange of, e.g., models or concepts and makes developments easier replicable. Nonetheless, robust and reliable evaluation and assessment of DL models has often proven challenging. This is at odds with their increasing safety relevance, which recently culminated in the field of \u201ctrustworthy ML\u201d. We believe that, among others, further unification of evaluation and safeguarding methodologies in terms of toolkits, i.e. small and specialized framework derivatives, might positively impact problems of trustworthiness as well as reproducibility. To this end, we present the first survey on toolkits for uncertainty estimation (UE) in DL, as UE forms a cornerstone in assessing model reliability. We investigate 11 toolkits with respect to modeling and evaluation capabilities, providing an in-depth comparison for the 3 most promising ones, namely Pyro, Tensorflow Probability, and Uncertainty Quantification 360. While the first two provide a large degree of flexibility and seamless integration into their respective framework, the last one has the larger methodological scope.", 
    "pdf": "SurveyOnUncertaintyToolkitsForDeepLearning.pdf",
    "video": "",
    "poster": "https://drive.google.com/file/d/1Uo9Hh0PfcE5-kg12MCtWKTwKVJYO9gGQ/view?usp=sharing"}, 
    {"id": 20, 
    "oral": "true",
    "title": "Integrating Rankings into Quantized Scores in Peer Review", "abstract": "In peer review, reviewers are usually asked to provide scores for the papers. The scores are then used by Area Chairs or Program Chairs in various ways in the decision-making process. The scores are usually elicited in a quantized form to accommodate the limited cognitive ability of humans to describe their opinions in numerical values. It has been found that the quantized scores suffer from a large number of ties, thereby leading to a significant loss of information. To mitigate this issue, conferences have started to ask reviewers to additionally provide a ranking of the papers they have reviewed. There are however two key challenges. First, there is no standard procedure for using this ranking information and Area Chairs may use it in different ways (including simply ignoring them), thereby leading to arbitrariness in the peer-review process. Second, there are no suitable interfaces for judicious use of this data nor methods to incorporate it in existing workflows, thereby leading to inefficiencies. We take a principled approach to integrate the ranking information into the scores. The output of our method is an updated score pertaining to each review that also incorporates the rankings. Our approach addresses the two aforementioned challenges by: (i) ensuring that rankings are incorporated into the updates scores in the same manner for all papers, thereby mitigating arbitrariness, and (ii) allowing to seamlessly use existing interfaces and workflows designed for scores. We empirically evaluate our method on synthetic datasets as well as on peer reviews from the ICLR 2017 conference, and find that it reduces the error by approximately 30\\% as compared to the best performing baseline on the ICLR 2017 data. ", 
    "authors": "Yusha Liu, Yichong Xu, Nihar B. Shah & Aarti Singh", 
    "pdf": "Integrating_Rankings_into_Quantized_Scores.pdf",
    "video": "",
    "poster": ""}, 
    {"id": 21, 
    "oral": "false",
    "title": "A Revealing Large-Scale Evaluation of Unsupervised Anomaly Detection Algorithms", "abstract": "Anomaly detection has many applications ranging from bank-fraud detection and cyber-threat detection to equipment maintenance and health monitoring. However, choosing a suitable algorithm for a given application remains a challenging design decision, often informed by the literature of anomaly detection algorithms. We extensively reviewed twelve of the most popular unsupervised anomaly detection methods. We observed that, so far, they have been compared using inconsistent protocols \u2013 the choice of the class of interest or the positive class, the split of training and test data, and choice of hyperparameters \u2013 leading to ambiguous evaluations. This observation led us to define a coherent evaluation protocol which we then used to produce an updated and more precise picture of the relative performance of the twelve methods on five widely used tabular datasets. While our evaluation cannot pinpoint a method that outperforms all the others on all datasets, it identifies those that stand out and revise misconceived knowledge about their relative performances.\n", 
    "authors": "Maxime Alvarez, Jean-Charles Verdier, D'Jeff Kanda Nkashama, Marc Frappier, Pierre Martin Tardif & Froduald Kabanza", 
    "pdf": "A_Revealing_Large-Scale_Evaluation_of_Unsupervised_Anomaly_Detection_Algorithms_-_V1.4.pdf",
    "video": "https://www.youtube.com/watch?v=oVrrwwKHlGo",
    "poster": "https://drive.google.com/file/d/1gYSIRGQSL244SEXODWsxjNPhxg1eouT0/view?usp=sharing"}, 
    {"id": 22, 
    "oral": "false",
    "title": "Rethinking Streaming Machine Learning Evaluation", "abstract": "While most work on evaluating machine learning (ML) models focuses on batches of data, computing the same metrics in a streaming setting (i.e., unbounded, timestamp-ordered datasets) fails to accurately identify when models are performing unexpectedly. In this position paper, we discuss how sliding windows--that ML metrics are evaluated over--can be negatively affected by real-world phenomena (e.g., delayed arrival of labels) and propose additional metrics to assess streaming ML performance. ", 
    "authors": "Shreya Shankar, Bernease R Herman & Aditya Parameswaran", 
    "pdf": "ml_eval_workshop",
    "note": "camera_ready_paper_missing",
    "video": "",
    "poster": ""},
    {"id": 24, 
    "oral": "true",
    "title": "Tradeoffs in Preventing Manipulation in Paper Bidding for Reviewer Assignment", "abstract": "Many conferences rely on paper bidding as a key component of their reviewer assignment procedure. These bids are then taken into account when assigning reviewers to help ensure that each reviewer is assigned to suitable papers. However, despite the benefits of using bids, reliance on paper bidding can allow malicious reviewers to manipulate the paper assignment for unethical purposes (e.g., getting assigned to a friend's paper). Several different approaches to preventing this manipulation have been proposed and deployed. In this paper, we enumerate certain desirable properties that algorithms for addressing bid manipulation should satisfy. We then offer a preliminary analysis of various approaches along with directions for future investigation. ", 
    "authors": "Steven Jecmen, Nihar B. Shah, Fei Fang & Vincent Conitzer", 
    "pdf": "malicious_bids_submit.pdf",
    "video": "https://drive.google.com/file/d/1VGo0RnDTZzh5iyO3QrvXDXCKtChan4O4/view?usp=sharing",
    "poster": ""}, 
    {"id": 25, 
    "oral": "false",
    "title": "Towards Yet Another Checklist for New Datasets", "abstract": "The goal of this opinion paper is to start (or more accurately, continue) a conversation on a checklist-style artifact for researchers to use when introducing a new dataset. While other checklists exist and are widely used, this paper proposes the inclusion of checklist-style questions to encourage dataset developers (as well as consumers) to consider data quality, diversity, and evaluation, especially as it relates to machine learning model robustness and generalizability", 
    "authors": "Stefan Larson", 
    "pdf": "iclr2022workshop.pdf",
    "video": "https://youtu.be/0rYBoE9BCW0",
    "poster": "https://drive.google.com/file/d/1aSoW-uYa51Vb3tHWlyBC2cvQjIOd33Yd/view?usp=sharing"}, 
    {"id": 26, 
    "oral": "false",
    "title": "Are Ground Truth Labels Reproducible? An Empirical Study", "abstract": "Standard evaluation techniques in Machine Learning (ML) and the corresponding statistical inference do not explicitly consider ground truth labels as a source of uncertainty, i.e., they assume that the benchmarks are reproducible. We investigate the reliability of ground truth labels in nine highly cited evaluation datasets. Via replication, we find the majority votes in three datasets to have zero reliability. They cannot be reproduced. The cause of irreproducibility is excessive rater disagreement, as evidenced in the zero inter-rater reliability. Contrary to popular belief, majority voting fails to have a material impact in this case. We conduct a smaller pilot using raters with high qualifications and find significant improvement in reliability across the board. This suggests high quality data collection is still paramount, and cannot be replaced by aggregation. We urge researchers, reviewers, and the publication processes (such as reproducibility checklists) to encourage the measurement and reporting of reproducibility in the ground truth data used in ML evaluation. Towards this end, we publish and release all the replications and associated data to aid assessment of reproducibility of this work.", 
    "authors": "Ka Wong, Praveen Paritosh & Kurt Bollacker", 
    "pdf": "GroundTruthReproducibilityICLRSubmitted.pdf",
    "note": "camera_ready_paper_missing",
    "video": "",
    "poster": ""},
    {"id": 27, 
    "oral": "false",
    "title": "Incentivizing Empirical Science in Machine Learning: Problems and Proposals",
    "abstract": "We introduce a proposal to help address a structural problem in ML publishing: the lack of community support and perceived lack of legitimacy for experimental scientific work that neither proves a mathematical theorem, nor improves a practical application. Such experimental work is the bedrock of many fields of science,  yet is not well appreciated by top ML publication venues (e.g. NeurIPS, ICML, ICLR). The problem is twofold: reviewers are often unaware of the value of such work, and thus authors are disincentivized from producing and submitting such work. The result is a suffocation of a scientific methodology that has a long history of success in the natural sciences, and has recently been fruitful in machine learning. To address this, we propose introducing a Best Paper Award specifically for foundational experimental work in machine learning. The award targets scientific work that is missed by existing communities: we exclude primarily theoretical work and application-motivated work, both of which are well supported by existing venues (e.g. COLT, CVPR). We propose that ML venues include a subject-area for ``scientific aspects of machine learning'', and consider papers in this subject for the award. More ambitiously, it can be implemented as an endowed yearly award considering all papers in the prior year. We expect that establishing an award will help legitimize this research area, establish standards for such scientific work, and encourage authors to conduct this work with the support of the community. In this proposal, we first discuss the structural problems in ML publication which we hope to address. We then present a call-for-papers for the ``science of ML'' subject area, describing the type of work we want to encourage. We argue that it is not only a scientifically legitimate type of work, but perhaps even a necessary type of work. Finally, we discuss guidelines for how such papers may be evaluated by reviewers.", 
    "authors": "Preetum Nakkiran & Mikhail Belkin", 
    "pdf": "science_ml_proposal_2am.pdf",
    "video": "",
    "poster": ""}, 
    {"id": 29, 
    "oral": "false",
    "title": "Why External Validity Matters for Machine Learning Evaluation: Motivation and Open Problems", "abstract": "New machine learning methods often fail to perform as expected on datasets similar to benchmarks reported on in their respective papers. These performance gaps pose a challenge for evaluation: both researchers and practitioners expect (or hope) that machine learning models which perform well on a dataset designed for a task perform well on other datasets matched to that task. We argue that external validity, the relationships between tasks and the learning problems which instantiate them, is understudied. We highlight the ways in which algorithm developers and benchmark creators fail to address this concern of external validity, suggest some remedies, and identify open questions in external validity which would help the community better build benchmarks and understand model performance.", 
    "authors": "Thomas I. Liao, Rohan Taori & Ludwig Schmidt", 
    "pdf": "ml_eval_2021.pdf",
    "video": "https://drive.google.com/file/d/1rQHv3MvvpdPC5TnGpiXDAHhIhafwcWYo/view?usp=sharing",
    "poster": "https://drive.google.com/file/d/18baNWrgEsHBxNLkwPiWbCdzmzPVlNoWY/view?usp=sharing"}, 
    {"id": 30, 
    "oral": "false",
    "title": "A Quality-Diversity-based Evaluation Strategy for Symbolic Music Generation", "abstract": "Since human (audience) evaluation methods are challenging due to their logistic inconvenience, symbolic music generation systems are typically evaluated using loss-based measures or some statistical analyses with pre-defined musical metrics. Even though these loss-based and statistical methods could be informative to some extent, they often cannot guarantee any success for the generative model in terms of higher-level musical qualities, such as style / genre. Also, as another aspect of evaluation, diversity of the generated material is not considered for symbolic music generators. In this study, we argue that Quality-Diversity-based evaluation approach is more appropriate to value symbolic music generators. We give a few examples of where loss-based and statistical methods fail and suggest some techniques for quality-based and diversity-based evaluation, jointly forming a Quality-Diversity-based evaluation strategy.", 
    "authors": "Berker Banar & Simon Colton", 
    "pdf": "A_Quality_Diversity_Based_Evaluation_Strategy_For_Symbolic_Music_Generation.pdf",
    "video": "https://www.youtube.com/watch?v=6AYH8KAmyp8",
    "poster": "https://drive.google.com/file/d/1yzhQqhu2N7YML1MQz-pYvB82ZeEKYGlT/view?usp=sharing"}]
